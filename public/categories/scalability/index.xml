<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>scalability on Aaron's Blog</title><link>https://www.aaronheld.com/categories/scalability/</link><description>Recent content in scalability on Aaron's Blog</description><generator>Hugo -- gohugo.io</generator><copyright>Copyright © 2008–2018, Steve Francia and the Hugo Authors; all rights reserved.</copyright><lastBuildDate>Mon, 14 Jul 2008 00:00:00 +0000</lastBuildDate><atom:link href="https://www.aaronheld.com/categories/scalability/index.xml" rel="self" type="application/rss+xml"/><item><title>reminiscing the cache wars</title><link>https://www.aaronheld.com/post/reminiscing-the-cache-wars/</link><pubDate>Mon, 14 Jul 2008 00:00:00 +0000</pubDate><guid>https://www.aaronheld.com/post/reminiscing-the-cache-wars/</guid><description>
&lt;p>Recently at work people are discussing the merits of different cache servers.  That brought back memories of my days as the R&amp;amp;D Product lead for a line of cache systems.&lt;/p>
&lt;p>The high point was participating in the two week &lt;a href="http://polygraph.ircache.net/Results/cacheoff-2/">&amp;quot;Cache Bake Off&amp;quot;&lt;/a> hosted by NLANR (the team broke off and formed &lt;a href="http://www.measurement-factory.com">The Measurement Factory&lt;/a>.  This was great fun where engineering teams from major companies got together to have thier systems pounded in a no holds barred performance test. I was working for a hardware company that had thier sights firmly set of being the leading tier two vendor (Tier one was considered too competative to take exponential growth risks).  I think my gear nailed it by achieving 80% of the performance of the leadning brand at 1/5 the price.  (And you could cluster 2 for less then half the cost and have over 50% better performance - if you order now you can get fee overnight shipping).&lt;/p>
&lt;p>The test was run using software called web-polygraph running a 'workload'&lt;/p>
&lt;p>The workload was considered to represent standard web traffic and put together by some sharp guys.&lt;/p>
&lt;blockquote>
&lt;p>One difficult part of this benchmark (and indeed most) was to develop the proper workload. Real-world Web traffic is incredibly complicated, both to understand and to simulate. Many workload attributes are well understood by themselves, but not when combined with each other. For example, we have a clear idea of real-world object size distributions. But how does object size combine with popularity and content-type? Are popular HTML objects larger, smaller, or the same as unpopular ones?&lt;/p>
&lt;/blockquote>
&lt;p>The per page breakdown was: &lt;code>Type Percentage Reply Size Cachability Image 65.0% exp(4.5KB) 80% HTML 15.0% exp(8.5KB) 90% Download 0.5% logn(300KB,300KB) 95% Other 19.5% logn(25KB,10KB) 72%&lt;/code>&lt;/p>
&lt;p>Surprisingly this mix is pretty close to our current page patterns.&lt;/p>
&lt;p>Probably the best part about this test was the scientific way it was run.  There was no marketing aspect to in the charts and numbers that got published and once you entered you agreed to let them make your results known to the world - good or bad.&lt;/p>
&lt;p>I wonder how many companies now would have enough faith in thier offering to allow objective test data to be released without markting/PR oversight?&lt;/p></description></item></channel></rss>